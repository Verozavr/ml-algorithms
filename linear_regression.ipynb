{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e149ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear regression class.\n",
    "\n",
    "    Typical use:\n",
    "        model = MyLineReg(n_iter=50, learning_rate=0.15, metric=\"mae\", reg=\"l1\", l1_coef=0.25)\n",
    "        model.fit(x=x, y=y, verbose=10)\n",
    "        predictions = model.predict(samples=samples)\n",
    "\n",
    "    Attributes:\n",
    "        - reg: Type of regularization (\"l1\" for Lasso, \"l2\" for Ridge, \"elasticnet\" for ElsticNet).\n",
    "        - metric: Metric for testing model (\"mae\" for Mean Absolute Error, \"mse\" for Mean Squared Error, \"rmse\" for Root Mean Squared Error, \"r2\" for Coefficient of Determination, \"mape\" for Mean Absolute Percentage Error).\n",
    "        - _metric_value: The best metric score (If metric is not speicified, Loss function score is calculated).\n",
    "        - n_iter: Count of gradient iterations.\n",
    "        - _weights: Numpy array of model weights.\n",
    "        - l1_coef: Coefficient of Lasslo regularization.\n",
    "        - l2_coef: Coefficient of Ridge regularization.\n",
    "        - learning_rate: Multiplier of gradient step (can be lambda function).\n",
    "\n",
    "    Methods:\n",
    "        - get_coef: Returns weights of a fitting model.\n",
    "        - fit: Fitting a model to find the best weights (logging evry n step when verbose = n is specified).\n",
    "        - get_best_score: Returns the best model metric score.\n",
    "        - predict: Returns a numeric array of predicted target values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 learning_rate: float = 0.1, \n",
    "                 n_iter: int = 100, \n",
    "                 metric: str = None, \n",
    "                 reg: str = None, \n",
    "                 l1_coef: float = 0, \n",
    "                 l2_coef: float = 0,\n",
    "                 sgd_sample: float = None,\n",
    "                 random_state: int = 42) -> None:\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self._weights = None\n",
    "        self.metric = metric\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return f\"{__class__.__name__} class: n_iter={self.n_iter}, learning_rate={self.learning_rate}\" \n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose = False) -> None:\n",
    "        #Reset index for X dataset\n",
    "        X.reset_index(drop=True)\n",
    "        \n",
    "        # Fill the first column of feature matrix with \"1\" values (for intercept)\n",
    "        X.insert(loc = 0, column = 'x0', value = 1)\n",
    "        \n",
    "        num_observations, num_features = X.shape\n",
    "        \n",
    "        # Set initial weights equal to 1\n",
    "        weights = [1] * num_features\n",
    "        \n",
    "        # Convert sgd_sample share to integer\n",
    "        if self.sgd_sample and self.sgd_sample <= 1:\n",
    "            self.sgd_sample = round(self.sgd_sample*num_observations)\n",
    "            \n",
    "        \n",
    "        # Fix random seed\n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        # Make iterations\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            \n",
    "            # Select the subset for calculations \n",
    "            if self.sgd_sample:\n",
    "                sample_rows_idx = random.sample(range(num_observations), self.sgd_sample)\n",
    "                subset_num_observations = self.sgd_sample\n",
    "                subset_X = X.iloc[sample_rows_idx]\n",
    "                subset_y = y[sample_rows_idx]\n",
    "\n",
    "            else:\n",
    "                subset_num_observations = num_observations\n",
    "                subset_X = X\n",
    "                subset_y = y\n",
    "            \n",
    "            # Make predictions and calculate errors\n",
    "            y_predicted = X.dot(weights)\n",
    "            errors = y_predicted - y\n",
    "            \n",
    "            # Calculate Loss function\n",
    "            if self.reg:\n",
    "                match self.reg:\n",
    "                    case \"l1\":\n",
    "                        Loss_function = sum(errors ** 2) / num_observations + self.l1_coef * sum(np.abs(weights))\n",
    "                    case \"l2\":\n",
    "                        Loss_function =  sum(errors ** 2) / num_observations + 2 * self.l2_coef * sum(np.array(weights)**2)\n",
    "                    case \"elasticnet\":\n",
    "                        Loss_function = sum(errors ** 2) / num_observations + self.l1_coef * sum(np.abs(weights)) + + 2*self.l2_coef*sum(np.array(weights)**2)\n",
    "            \n",
    "            # When regularization is not defined MSE is a Loss function\n",
    "            else:\n",
    "                Loss_function = sum(errors ** 2) / num_observations\n",
    "            \n",
    "            # Calculate metric\n",
    "            if self.metric:\n",
    "                match self.metric:\n",
    "                    case \"mae\":\n",
    "                        metric_temp_value = sum(abs(errors)) / num_observations\n",
    "                    case \"mse\":\n",
    "                        metric_temp_value = sum(errors ** 2) / num_observations\n",
    "                    case \"rmse\":\n",
    "                        metric_temp_value = sqrt(sum(errors ** 2) / num_observations)\n",
    "                    case \"mape\":\n",
    "                        metric_temp_value = (100 / num_observations) * sum(abs(errors / y))\n",
    "                    case \"r2\":\n",
    "                        mean_y = statistics.mean(y)\n",
    "                        metric_temp_value = 1 - sum(errors ** 2) / sum((y - mean_y) ** 2)\n",
    "                        \n",
    "                self._metric_value = metric_temp_value\n",
    "            else:\n",
    "                self._metric_value = Loss_function\n",
    "                \n",
    "            # Log after verbose iterations \n",
    "            if verbose and i % verbose == 0:\n",
    "                \n",
    "                if self.metric:\n",
    "                    print(f'{i} | loss: {Loss_function} | {self.metric}: {metric_temp_value}')\n",
    "                         \n",
    "                else:\n",
    "                    print(f'{i} | loss: {Loss_function}')\n",
    "            \n",
    "            # Make subset predictions and calculate subset errors\n",
    "            subset_y_predicted = subset_X.dot(weights)\n",
    "            subset_errors = subset_y_predicted - subset_y\n",
    "            \n",
    "            # Calculate gradient based on subset\n",
    "            if self.reg:\n",
    "                match self.reg:\n",
    "                    case \"l1\":\n",
    "                        gradient = 2 /  subset_num_observations *  subset_X.T.dot(subset_errors) + self.l1_coef * np.sign(weights)\n",
    "                    case \"l2\":\n",
    "                        gradient = 2 /  subset_num_observations *  subset_X.T.dot(subset_errors) + 2 * self.l2_coef * np.array(weights)\n",
    "                    case \"elasticnet\":\n",
    "                        gradient = 2 /  subset_num_observations *  subset_X.T.dot(subset_errors) + self.l1_coef * np.sign(weights) + 2*self.l2_coef*np.array(weights)\n",
    "            \n",
    "            else:\n",
    "                gradient = 2 /  subset_num_observations *  subset_X.T.dot( subset_errors)\n",
    "            \n",
    "            # Calculate new weights\n",
    "            if callable(self.learning_rate):\n",
    "                weights -= self.learning_rate(i) * gradient\n",
    "            else:\n",
    "                weights -= self.learning_rate * gradient\n",
    "        \n",
    "        # Final (best) weights\n",
    "        self.weights = weights  \n",
    "\n",
    "\n",
    "    \n",
    "    def get_coef(self) -> np.array:\n",
    "        return np.array(self.weights[1:])\n",
    "    \n",
    "    def predict(self, X) -> np.array:\n",
    "        \n",
    "        # Fill the first column of feature matrix with \"1\" values (for intercept)\n",
    "        X.insert(loc = 0, column = 'x0', value = 1)\n",
    "        y_predicted = X.dot(self.weights)\n",
    "        return np.array(y_predicted)\n",
    "    \n",
    "    \n",
    "    def get_best_score(self) -> float:\n",
    "        return self._metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6bd51c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ficticious dataset\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, n_informative=3, noise=5, random_state=42)\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "X.columns = [f'col_{col_number}' for col_number in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d1170e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class instance\n",
    "sample_one = MyLineReg(n_iter = 50, sgd_sample = 0.3, learning_rate = 0.1, metric = \"mae\", reg = \"l1\", l1_coef = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "39575999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 | loss: 407.7536252194835 | mae: 15.586722346130946\n",
      "20 | loss: 83.2392782476568 | mae: 4.849404419651548\n",
      "30 | loss: 71.10224922935954 | mae: 3.8808217054231546\n",
      "40 | loss: 70.41575391812177 | mae: 3.7501823824164204\n",
      "50 | loss: 70.49019070453042 | mae: 3.7645359834704584\n"
     ]
    }
   ],
   "source": [
    "sample_one.fit(X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "234ed503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57.07398193, 35.0790659 ,  0.14585922, 63.40005449, -0.2202788 ])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_one.get_coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "621c0bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7645359834704584"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_one.get_best_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
