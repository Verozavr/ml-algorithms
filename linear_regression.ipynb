{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "90e149ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear regression class.\n",
    "\n",
    "    Typical use:\n",
    "        model = LinearRegression(n_iter=50, learning_rate=0.15, metric=\"mae\", reg=\"l1\", l1_coef=0.25)\n",
    "        model.fit(x=x, y=y, verbose=10)\n",
    "        predictions = model.predict(samples=samples)\n",
    "\n",
    "    Attributes:\n",
    "        - reg: Type of regularization (\"l1\" for Lasso, \"l2\" for Ridge, \"elasticnet\" for ElsticNet).\n",
    "        - metric: Metric for testing model (\"mae\" for Mean Absolute Error, \"mse\" for Mean Squared Error, \"rmse\" for Root Mean Squared Error, \"r2\" for Coefficient of Determination, \"mape\" for Mean Absolute Percentage Error).\n",
    "        - _metric_value: The best metric score (If metric is not speicified, Loss function score is calculated).\n",
    "        - n_iter: Count of gradient iterations.\n",
    "        - _weights: Numpy array of model weights.\n",
    "        - l1_coef: Coefficient of Lasslo regularization.\n",
    "        - l2_coef: Coefficient of Ridge regularization.\n",
    "        - learning_rate: Multiplier of gradient step (can be lambda function)\n",
    "        - sgd_sample: Number or share of observations making up Mini-Batch for stochastic gradient descent\n",
    "        - random_state: Seed for forming the same Mini-Batches\n",
    "\n",
    "    Methods:\n",
    "        - get_coef: Returns weights of a fitting model.\n",
    "        - fit: Fitting a model to find the best weights (logging evry n step when verbose = n is specified).\n",
    "        - get_best_score: Returns the best model metric score.\n",
    "        - predict: Returns a numeric array of predicted target values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float | Callable[[int], float] = 0.1,\n",
    "        n_iter: int = 100,\n",
    "        metric: str | None = None,\n",
    "        reg: str | None = None,\n",
    "        l1_coef: float = 0,\n",
    "        l2_coef: float = 0,\n",
    "        sgd_sample: float | None = None,\n",
    "        random_state: int = 42,\n",
    "    ) -> None:\n",
    "        # Raise Error if sgd_sample is not valid\n",
    "        if sgd_sample is not None:\n",
    "            if sgd_sample > 1 and not isinstance(sgd_sample, int):\n",
    "                raise ValueError(\n",
    "                    \"'sgd_sample' must be either number of observations (integer) or share of observations (float between 0 and 1)\"\n",
    "                )\n",
    "            if sgd_sample <= 0:\n",
    "                raise ValueError(\n",
    "                    \"'sgd_sample' can not be negative value! 'sgd_sample' must be either number of observations (integer) or share of observations (float between 0 and 1)\"\n",
    "                )\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self._weights = None\n",
    "        self.metric = metric\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{__class__.__name__} class: n_iter = {self.n_iter}, learning_rate = {self.learning_rate}\"\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, verbose=False) -> None:\n",
    "        X = deepcopy(X)\n",
    "\n",
    "        # Reset index for X dataset\n",
    "        X.reset_index(drop=True)\n",
    "\n",
    "        # Fill the first column of feature matrix with \"1\" values (for intercept)\n",
    "        X.insert(loc=0, column=\"intercept\", value=1)\n",
    "\n",
    "        num_observations, num_features = X.shape\n",
    "\n",
    "        # Set initial weights equal to 1\n",
    "        weights = [1] * num_features\n",
    "\n",
    "        # Convert sgd_sample share to integer\n",
    "        if self.sgd_sample and self.sgd_sample <= 1:\n",
    "            self.sgd_sample = round(self.sgd_sample * num_observations)\n",
    "\n",
    "        # Fix random seed\n",
    "        random.seed(self.random_state)\n",
    "\n",
    "        # Make iterations to update weights\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            # Calculate predictions and errors on the whole dataset for displayed metric\n",
    "            y_predicted = X.dot(weights)\n",
    "            errors = y_predicted - y\n",
    "\n",
    "            MSE = sum(errors**2) / num_observations\n",
    "\n",
    "            # Calculate Loss function\n",
    "            if self.reg:\n",
    "                match self.reg:\n",
    "                    case \"l1\":\n",
    "                        Loss_function = MSE + self.l1_coef * sum(np.abs(weights))\n",
    "                    case \"l2\":\n",
    "                        Loss_function = MSE + 2 * self.l2_coef * sum(\n",
    "                            np.array(weights) ** 2\n",
    "                        )\n",
    "                    case \"elasticnet\":\n",
    "                        Loss_function = (\n",
    "                            MSE\n",
    "                            + self.l1_coef * sum(np.abs(weights))\n",
    "                            + 2 * self.l2_coef * sum(np.array(weights) ** 2)\n",
    "                        )\n",
    "\n",
    "            # When regularization is not defined MSE is a Loss function\n",
    "            else:\n",
    "                Loss_function = MSE\n",
    "\n",
    "            # Calculate metric\n",
    "            if self.metric:\n",
    "                match self.metric:\n",
    "                    case \"mae\":\n",
    "                        metric_temp_value = sum(abs(errors)) / num_observations\n",
    "                    case \"mse\":\n",
    "                        metric_temp_value = sum(errors**2) / num_observations\n",
    "                    case \"rmse\":\n",
    "                        metric_temp_value = np.sqrt(sum(errors**2) / num_observations)\n",
    "                    case \"mape\":\n",
    "                        metric_temp_value = (100 / num_observations) * sum(\n",
    "                            abs(errors / y)\n",
    "                        )\n",
    "                    case \"r2\":\n",
    "                        mean_y = sum(y) / len(y)\n",
    "                        metric_temp_value = 1 - sum(errors**2) / sum(\n",
    "                            (y - mean_y) ** 2\n",
    "                        )\n",
    "\n",
    "                self._metric_value = metric_temp_value\n",
    "            else:\n",
    "                self._metric_value = Loss_function\n",
    "\n",
    "            # Log after verbose iterations\n",
    "            if verbose and i % verbose == 0:\n",
    "                if self.metric:\n",
    "                    print(\n",
    "                        f\"{i} | loss: {Loss_function} | {self.metric}: {metric_temp_value}\"\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    print(f\"{i} | loss: {Loss_function}\")\n",
    "\n",
    "            # Select the subset for calculations\n",
    "            if self.sgd_sample:\n",
    "                sample_rows_idx = random.sample(\n",
    "                    range(num_observations), self.sgd_sample\n",
    "                )\n",
    "                subset_num_observations = self.sgd_sample\n",
    "                subset_X = X.iloc[sample_rows_idx]\n",
    "                subset_y = y[sample_rows_idx]\n",
    "\n",
    "            else:\n",
    "                subset_num_observations = num_observations\n",
    "                subset_X = X\n",
    "                subset_y = y\n",
    "\n",
    "            # Make subset predictions and calculate subset errors\n",
    "            subset_y_predicted = subset_X.dot(weights)\n",
    "            subset_errors = subset_y_predicted - subset_y\n",
    "\n",
    "            # Calculate gradient based on subset\n",
    "            if self.reg:\n",
    "                match self.reg:\n",
    "                    case \"l1\":\n",
    "                        gradient = 2 / subset_num_observations * subset_X.T.dot(\n",
    "                            subset_errors\n",
    "                        ) + self.l1_coef * np.sign(weights)\n",
    "                    case \"l2\":\n",
    "                        gradient = 2 / subset_num_observations * subset_X.T.dot(\n",
    "                            subset_errors\n",
    "                        ) + 2 * self.l2_coef * np.array(weights)\n",
    "                    case \"elasticnet\":\n",
    "                        gradient = (\n",
    "                            2 / subset_num_observations * subset_X.T.dot(subset_errors)\n",
    "                            + self.l1_coef * np.sign(weights)\n",
    "                            + 2 * self.l2_coef * np.array(weights)\n",
    "                        )\n",
    "\n",
    "            else:\n",
    "                gradient = 2 / subset_num_observations * subset_X.T.dot(subset_errors)\n",
    "\n",
    "            # Calculate new weights\n",
    "            if callable(self.learning_rate):\n",
    "                weights -= self.learning_rate(i) * gradient\n",
    "            else:\n",
    "                weights -= self.learning_rate * gradient\n",
    "\n",
    "        # Final (best) weights\n",
    "        self.weights = weights\n",
    "\n",
    "    def get_coef(self) -> np.array:\n",
    "        return np.array(self.weights[1:])\n",
    "\n",
    "    def predict(self, X) -> np.array:\n",
    "        X = deepcopy(X)\n",
    "\n",
    "        # Fill the first column of feature matrix with \"1\" values (for intercept)\n",
    "        X.insert(loc=0, column=\"intercept\", value=1)\n",
    "        y_predicted = X.dot(self.weights)\n",
    "        return np.array(y_predicted)\n",
    "\n",
    "    def get_best_score(self) -> float:\n",
    "        return self._metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6bd51c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ficticious dataset\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(\n",
    "    n_samples=100, n_features=5, n_informative=3, noise=5, random_state=42\n",
    ")\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)\n",
    "X.columns = [f\"col_{col_number}\" for col_number in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0d1170e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class instance\n",
    "\n",
    "sample_one = LinearRegression(\n",
    "    n_iter=50,\n",
    "    sgd_sample=0.3,\n",
    "    learning_rate=lambda iter: 0.5 * (0.6**iter),\n",
    "    metric=\"r2\",\n",
    "    reg=\"l1\",\n",
    "    l1_coef=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "39575999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 | loss: 371.7432225528488 | r2: 0.9541323715174207\n",
      "20 | loss: 365.80716875013707 | r2: 0.9549647736147504\n",
      "30 | loss: 365.7708793908338 | r2: 0.9549698691429512\n",
      "40 | loss: 365.77064185449285 | r2: 0.954969902460749\n",
      "50 | loss: 365.770640440973 | r2: 0.954969902658895\n"
     ]
    }
   ],
   "source": [
    "sample_one.fit(X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "234ed503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46.92436695, 32.63855279, -4.57457334, 47.54099476, -4.91735423])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_one.get_coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "621c0bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.954969902658895"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_one.get_best_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
